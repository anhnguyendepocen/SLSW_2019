---
title: "Logistic Regression"
author: "Peter Freeman (2019 SLSW)"
date: "1 July 2019"
output: 
  slidy_presentation:
    font_adjustment: -1
---

The Setting
===

<center>![](http://www.stat.cmu.edu/~pfreeman/Figure_4.2.png){width=80%}</center>

(Figure 4.2, *Introduction to Statistical Learning* by James et al.)

- To the left is a linear regression fit. It is not limited to lie within the range [0,1].

- To the right is a logisitic regression fit.

Generalized Linear Models
===

Having discussed linear regression, it makes sense to first step back and look at generalized linear models (or GLMs) and then look at one particular example of a generalized linear model: *logistic regression*.

In conventional linear regression, we estimate the mean value of the response variable $Y$, given predictor variables $X_1,\ldots,X_p$:
$$
E[Y|X] = \beta_0 + \beta_1 X_1 + \cdots + \beta_n X_n \,.
$$
In a generalized linear model, we include a "link function" $g$ that takes the linear model and transforms it:
$$
g(E[Y|X]) = \beta_0 + \beta_1 X_1 + \cdots + \beta_n X_n \,.
$$
One uses the link function to reduce the range of possible values for $E[Y \vert X]$ from $(-\infty,\infty)$ to, e.g., [0,1] or $[0,\infty)$, etc. 

In addition, in a GLM you specify a "family," or the distribution that governs the observed response values. For instance, if the observed response values are zero and the positive integers, the family could be "Poisson." If they are just 0 and 1, the family is "binomial." Etc.

Logistic Regression
===

For logistic regression, a conventional choice of link function is the *logit* function:
$$
\log\left[\frac{E[Y \vert X]}{1-E[Y \vert X]}\right] = \beta_0 + \beta_1 X_1 + \cdots + \beta_n X_n \,,
$$
so that
$$
E[Y \vert X] = \frac{e^{\beta_0 + \beta_1 X_1 + \cdots + \beta_n X_n}}{1 + e^{\beta_0 + \beta_1 X_1 + \cdots + \beta_n X_n}} \,.
$$

Assuming that we are dealing with two classes, the possible observed values for $Y$ are 0 and 1, so the family is `binomial`, i.e.,
$$
Y \vert X \sim {\rm Binomial}(n=1,p=E[Y\vert X]) \,.
$$

A major difference between linear and logistic regression is that the latter involves numerical optimization, i.e., instead of plugging into a formula, you have to use an iterative algorithm to find the $\beta$'s that maximize the likelihood function:
$$
\left( \prod_{i: Y_i=1} E[Y \vert X_i] \right) \left( \prod_{i: Y_i=0} (1 - E[Y \vert X_i]) \right) \,.
$$
Numerical optimization means the logistic regression runs more slowly than linear regression.

Logistic Regression: Example
===

The dataset that we read in below contains magnitude and redshift data for 500 quasars and 500 stars. The idea is to learn a classifier that can discriminate between quasars and stars with a low misclassification rate.
```{r}
df = read.csv("https://raw.githubusercontent.com/pefreeman/PSU_2019/master/StarQuasar.csv")
dim(df)
names(df)
set.seed(202)
s = sample(nrow(df),0.7*nrow(df))
pred.train = df[s,1:5]                                  # don't include redshift or redshift error!
pred.test  = df[-s,1:5]
resp.train = df[s,8]
resp.test  = df[-s,8]
```

Logistic Regression: Example
===

To more fully understand the output below, it helps to know that `R` by default orders factor variables in alphabetical order: `QSO` comes before `STAR`, so `QSO` == 0 and `STAR` == 1. You can override this default behavior and provide your own ordering if you wish to.
```{r}
out.log = glm(resp.train~.,data=pred.train,family=binomial)
resp.prob = predict(out.log,newdata=pred.test,type="response")
resp.pred = ifelse(resp.prob>0.5,"STAR","QSO")
mean(resp.pred!=resp.test)
table(resp.pred,resp.test)
```
We observe a 15% misclassification rate, with stars being misidentified as QSOs roughly as often as QSOs are misidentified as stars.
