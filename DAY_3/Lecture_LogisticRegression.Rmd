---
title: "Logistic Regression"
author: "Peter Freeman (2019 SLSW)"
date: "1 July 2019"
output: 
  slidy_presentation:
    font_adjustment: -1
---

The Setting
===

<center>![](http://www.stat.cmu.edu/~pfreeman/Figure_4.2.png){width=80%}</center>

(Figure 4.2, *Introduction to Statistical Learning* by James et al.)

Suppose your response is categorical and has two possible values (`TRUE` and `FALSE`, `Vanilla` and `Chocolate`, `Fiction` and `Non-Fiction`). We identify one value as "Class 0" and one as "Class 1." A baseline statistical model to learn the association between predictor variables and a binary response variable is *logistic regression*.

- To the left is a linear regression fit. It is not limited to lie within the range [0,1].

- To the right is a logisitic regression fit.

Generalized Linear Models
===

Having discussed linear regression, it makes sense to first step back and look at generalized linear models (or GLMs) and then look at one particular example of a generalized linear model: *logistic regression*.

In conventional linear regression, we estimate the mean value of the response variable $Y$, given predictor variables $X_1,\ldots,X_p$:
$$
E[Y|X_1,\ldots,X_p] = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p \,.
$$
In a generalized linear model, we include a "link function" $g$ that takes the linear model and transforms it:
$$
g(E[Y|X_1,\ldots,X_p]) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p \,.
$$
One uses the link function to reduce the range of possible values for $E[Y \vert X_1,\ldots,X_p]$ from $(-\infty,\infty)$ to, e.g., [0,1] or $[0,\infty)$, etc. 

In addition, in a GLM you specify a "family," or the distribution that governs the observed response values. For instance, if the observed response values are zero and the positive integers, the family could be "Poisson." If they are just 0 and 1, the family is "binomial." Etc.

Logistic Regression
===

For logistic regression, a conventional choice of link function is the *logit* function:
$$
\log\left[\frac{E[Y \vert X_1,\ldots,X_p]}{1-E[Y \vert X_1,\ldots,X_p]}\right] = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p \,,
$$
so that
$$
E[Y \vert X_1,\ldots,X_p] = \frac{e^{\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p}}{1 + e^{\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p}} \,.
$$

Assuming that we are dealing with two classes, the possible observed values for $Y$ are 0 and 1, so the family is `binomial`, i.e.,
$$
Y \vert X_1,\ldots,X_p \sim {\rm Binomial}(n=1,p=E[Y\vert X_1,\ldots,X_p]) \,.
$$

A major difference between linear and logistic regression is that the latter involves numerical optimization, i.e., instead of plugging into a formula, you have to use an iterative algorithm to find the $\beta$'s that maximize the likelihood function:
$$
\left( \prod_{i: Y_i=1} E[Y \vert X_{1,i},\ldots,X_{p,i}] \right) \left( \prod_{i: Y_i=0} (1 - E[Y \vert X_{1,i},\ldots,X_{p,i}]) \right) \,.
$$
Numerical optimization means the logistic regression runs more slowly than linear regression.

Logistic Regression: Inference
===

A major motivating factor underlying the use of logistic regression, and indeed all generalized linear models, is that one can perform inference...e.g., how does the response change when we change a predictor by one unit?

For linear regression, the answer to the question posed above is straightforward.

For logistic regression, it is a little less straightforward, because the predicted response varies non-linearly with the predictor variable values. One convention is to fall back upon the concept of "odds."

Let's say that the predicted response is 0.8 given a particular predictor variable value. (For simplicity, let's assume we have just one predictor variable.) That means that we expect that if we were to repeatedly sample response values given that predictor variable values, we would expect class 1 to appear four times as often as class 0:
$$
O = \frac{E[Y \vert X]}{1-E[Y \vert X]} = \frac{0.8}{1-0.8} = 4 = e^{\beta_0+\beta_1X} \,.
$$
Thus we say that for the given predictor variable value, the odds $O$ are 4 (or 4-1) in favor of class 1.

How does the odds change if I change the value of a predictor variable by one unit?
$$
e^{\beta_0+\beta_1(X+1)} = e^{\beta_0+\beta_1X}e^{\beta_1} = e^{\beta_1}O \,.
$$
For every unit change in $X$, the odds change by a factor $e^\beta_1$.

Logistic Regression: Example
===

The dataset that we read in below contains magnitude and redshift data for 500 quasars and 500 stars. The idea is to learn a classifier that can discriminate between quasars and stars with a low misclassification rate.
```{r}
df = read.csv("https://raw.githubusercontent.com/pefreeman/PSU_2019/master/StarQuasar.csv")
dim(df)
names(df)
set.seed(202)
s = sample(nrow(df),0.7*nrow(df))
pred.train = df[s,1:5]                                  # don't include redshift or redshift error!
pred.test  = df[-s,1:5]
resp.train = df[s,8]
resp.test  = df[-s,8]
```

Logistic Regression: Example
===

To more fully understand the output below, it helps to know that `R` by default orders factor variables in alphabetical order: `QSO` comes before `STAR`, so `QSO` == 0 and `STAR` == 1. You can override this default behavior and provide your own ordering if you wish to.
```{r}
out.log = glm(resp.train~.,data=pred.train,family=binomial)
resp.prob = predict(out.log,newdata=pred.test,type="response")
resp.pred = ifelse(resp.prob>0.5,"STAR","QSO")
mean(resp.pred!=resp.test)
table(resp.pred,resp.test)
```
We observe a 15% misclassification rate, with stars being misidentified as QSOs roughly as often as QSOs are misidentified as stars.
