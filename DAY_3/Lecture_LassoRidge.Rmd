---
title: "Lasso + Ridge Regression"
author: "Peter Freeman (2019 SLSW)"
date: "1 July 2019"
output: 
  slidy_presentation:
    font_adjustment: -1
---

Shrinkage Methods
===

The lasso and ridge regression are *shrinkage methods* that are alternatives to best subset selection (and to forward or backward stepwise selection). They differ from BSS in that they penalize models in a different manner than BSS does:
\begin{eqnarray*}
{\rm lasso:} &~& {\rm RSS} + \lambda \sum_{i=1}^p \vert \beta_i \vert \\
{\rm ridge:} &~& {\rm RSS} + \lambda \sum_{i=1}^p \beta_i^2
\end{eqnarray*}
The effects of the additive penalty (or regularization) terms is dictated by the magnitude of the tuning parameter $\lambda$. If $\lambda \rightarrow 0$, then the penalty terms have no effect, and thus the use of lasso and ridge regression is equivalent to simply performing regular old linear regression.

However, if $\lambda \rightarrow \infty$, then...

- lasso: to balance the large value of $\lambda$, all the linear regression coefficients shrink toward zero, but some go to zero more quickly than others; hence lasso performs its own version of subset selection

- ridge: to balance the large value of $\lambda$, all the coefficients shrink toward zero

Shrinkage Methods
===

<center> ![](http://www.stat.cmu.edu/~pfreeman/Lasso.png){width=39.5%}
![](http://www.stat.cmu.edu/~pfreeman/Ridge.png){width=40%} </center>

For example: note how the magnitude of the coefficient for `Income` trends as $\lambda \rightarrow \infty$.

In the context of lasso, the coefficient goes to zero at $\lambda ~ 1000$; if the optimum value of $\lambda$ is larger, then `Income` would not be included in the learned model.

In the context of ridge regression, the coefficient shrinks towards zero, but never actually reaches it. `Income` is always a variable in the learned model, regardless of the value of $\lambda$.

While it may seem obvious that lasso is the preferred learning model (since it performs variable selection), realize that ridge regression may yield a small test-set MSE! Depending on circumstance, you may prefer to utilize a model that has $p$ predictor variables, but yields better predictions, over a model that has $k < p$ predictor variables, but yields worse predictions.

Shrinkage Methods: Caveats
===

1. If you use either the lasso or ridge regression, you should *standardize* your data. While there is no unique way to standardize, the most common convention is to, within each column of data, compute and subtract off the sample mean, and compute and subtract off the sample standard deviation:
$$
\tilde{X}_i = \frac{X_i - \bar{X}_i}{s_{X,i}}
$$

(Note that if you utilize the `glmnet` package, standardization is performed by default.)

2. $\lambda$ is a tuning parameter. This means that you have to split your training data into training and validation sets, or perform cross-validation on the training data.

(Note that if you utilize the `glmnet` package, the `cv.glmnet()` package will perform the necessary cross-validation for you.)

Lasso: Example
===

```{r}
load(url("https://raw.githubusercontent.com/pefreeman/36-290/master/EXAMPLE_DATASETS/PHOTO_MORPH/photo_morph.Rdata"))
```
The `load()` function places two stored variables into the global environment: <tt>predictors</tt>, a data frame with 16 measurements for each of 3,419 galaxies, and <tt>response</tt>, a vector with 3,419 redshifts (i.e., metrics of physical distance). Because lasso (and ridge regression) is more useful/effective when $n \sim p$, we will analyze only the first 70 objects.
```{r}
pred.train = predictors[1:50,]
pred.test  = predictors[51:70,]
resp.train = response[1:50]
resp.test  = response[51:70]
```

Lasso: Example
===

Here we utilize the `glmnet` package. The functions of `glmnet` are a tad "prickly" in that they don't abide by the usual rules that apply to model functions in `R`, as you will see. Note that to perform ridge regression, one can perform all the same steps below, except changing the argument `alpha=1` to `alpha=0`. Also note that one can apply lasso and ridge regression in the context of, e.g., binary classification by adding the argument `family="binomial"` to `glmnet()` function calls.
```{r fig.height=6,fig.width=6,fig.align="center"}
if ( require(glmnet) == FALSE ) {
  install.packages("glmnet",repos="https://cloud.r-project.org")
  library(glmnet)
}
x = model.matrix(resp.train~.,pred.train)[,-1]
y = resp.train
out.lasso = glmnet(x,y,alpha=1)  # alpha = 0: ridge regression; alpha = 1: lasso
plot(out.lasso,xvar="lambda")
```

Lasso: Example
===

What is the optimal value of $\lambda$?
```{r fig.height=6,fig.width=6,fig.align="center"}
set.seed(301)  # cv.glmnet() performs random sampling...so set the seed!
cv = cv.glmnet(x,y,alpha=1)
plot(cv)
cv$lambda.min
coef(out.lasso,cv$lambda.min)
```

Lasso: Example
===

```{r}
x.test    = model.matrix(resp.test~.,pred.test)[,-1]
resp.pred = predict(out.lasso,s=cv$lambda.min,newx=x.test)
mean((resp.pred-resp.test)^2)
```
