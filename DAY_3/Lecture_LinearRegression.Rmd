---
title: "Linear Regression"
author: "Peter Freeman (2019 SLSW)"
date: "1 July 2019"
output: 
  slidy_presentation:
    font_adjustment: -1
---

The Setting
===

As stated last week, we can view data that we observe in the following way:
$$
{\rm observed~data}~ = ~{\rm signal}~ + ~{\rm noise}
$$
The "signal" is deterministic and something we wish to model, while the noise is probabilistic. Stated mathematically:
$$
Y = f(X) + \epsilon \,.
$$

In linear regression, we assume that $Y$ is related to the variables $X$ via the model
$$
Y = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p + \epsilon \,.
$$
The probabilistic component $\epsilon$ is a so-called random variable that is assumed to be normally distributed with mean zero and constant covariance matrix $\Sigma$.

Linear Regression: Why Use It?
===

1. It is inflexible, but readily interpretable. (If $X$ is a person's height, and it changes by one unit, then $Y$, the distance that he or she has thrown a tennis ball, changes by $\beta_1$ units, on average.) Note that it is not necessarily the case that there is an *a priori* belief that the $X$'s and $Y$ are exactly linearly related.

2. It is a fast model to learn: the $\beta$'s can be computed via formula, as opposed to via slower numerical optimization.

Linear Regression: Basic Points to Remember
===

1. We use regression to estimate *conditional means*:
$$
E[Y \vert X_1,\cdots,X_p]
$$
i.e., the average value of $Y$ *given* values $X_1,\cdots,X_p$.

2. If the $X$'s are measured with uncertainty, then the estimates of the $\beta$'s become *biased*, i.e., they differ on average from the true value, and trend towards zero as the uncertainties increase.

Linear Regression: Output
===

The output of linear regression are estimates of coefficients, estimates of uncertainty for those coefficients, and $p$ values (i.e., `Pr(>|t|)` in the output below).
```{r}
set.seed(303)
x = 1:10
y = x + rnorm(10,sd=0.5)
out.lm = lm(y~x)
summary(out.lm)
```
In this simple example, the coefficient for variable $x$ is estimated to be 0.996, and the estimated probability that one would observe a value of 0.996 or larger (or -0.996 or smaller) is $1.44 \times 10^{-8}$. Since this is less than the conventional decision threshold of 0.05, we conclude that the true value of the coefficient is not zero, i.e., there is a significant association between $x$ and $y$.

Linear Regression: Output
===

Caveats to keep in mind regarding $p$ values:

1. If the true value of a coefficient $\beta_i$ is equal to zero, then the $p$ value is sampled from a Uniform(0,1) distribution (i.e., it is just as likely to have value 0.45 as 0.16 or 0.84). Thus there's a 5% chance that you'll conclude there's a significant association between $x$ and $y$ even when there is none.

2. As the sample size $n$ gets large, the estimated coefficient uncertainty goes to zero, and *all* predictors are eventually deemed significant.

$\Rightarrow$ While the $p$ values might be informative, use variable selection methods (covered elsewhere) to determine which subset of the predictors should be included in your final model.

Linear Regression: Output
===

In addition to the $p$ values, you will note in the example output a value dubbed "Adjusted R-squared" (which has value 0.983). The adjusted $R^2$ has a value between 0 and 1 and is an estimate of the proportion of the variance of the data along the $y$-axis explained by the linear regression model. Adjusted $R^2$ provides intuition as to how good a linear model fits to the data, as opposed to the mean-squared error.
```{r fig.height=4.5,fig.width=4.5,fig.align="center"}
library(ggplot2)
df = data.frame(x,y,predict(out.lm))
names(df) = c("x","y","y.pred")
ggplot(data=df,mapping=aes(x=x,y=y)) + geom_point() + geom_point(mapping=aes(x=x,y=y.pred),color="red") +
  geom_line(mapping=aes(x=x,y=y.pred),linetype="dashed",color="red")
cat("Variance of y = ",var(y),"  Variance of predicted y = ",var(df$y.pred),"  Raw R^2 = ",var(df$y.pred)/var(y),"\n")
```

Linear Regression: Other Points to Keep In Mind
===

A proper discussion of each of the points given below could span one or more lectures in a conventional regression class! Here we just mention them, as caveats.

1. You may find it useful to transform your variables. (For instance, a distribution of values that is highly skew may be much less so if you transform them logarithmically.) Keep this in mind as you perform EDA.

2. Outliers may adversely affect your regression estimates. In a linear regression setting, outliers may be identified via the "Cook's distance." We offer no general heuristic regarding how to deal with outliers, other than you should scrupulously document how you deal with them!

3. Beware collinearity! Collinearity is when one predictor variable is linearly associated with another. Collinearity is not necessarily harmful outside a linear regression setting, but must be dealt with in linear regression analyses. The general process is to perform the linear regression fit, compute the "variance inflation factor" via the `vif()` function in the `car` package, remove a variable if its vif is greater than 5 (or 10), and repeat the fitting process until no more variables are to be removed.

Last Point: Do I Really Need to Split My Data?
===

If your entire analysis workflow involves fitting one linear regression model to your data, there is no need to split your data into training and test datasets. Just learn the model, and interpret it.

However, if you intend to learn multiple models (e.g., linear regression, a regression tree, random forest), then you should estimate the $\beta$'s using only the training data and you should generate predictions and compute the MSE using the test data, so as to be able to produce apples-to-apples comparisons of MSE values.

Last Point...Actually, a Slight Digression
===

How to define a model in `R`, via model formulae. Assume you have predictor variables $X_1$ and $X_2$ and response variable $Y$. Then

1. $Y \sim X_1$ means "regress $Y$ upon $X_1$ only."
2. $Y \sim X_1+X_2$ means "regress $Y$ upon both $X_1$ and $X_2$."
3. $Y \sim .$ means "regress $Y$ upon all predictor variables." Here that is the same as saying $Y \sim X_1+X_2$. 
4. $Y \sim .-X_2$ means "regress $Y$ upon all predictor variables except $X_2$." Here that is the same as saying $Y \sim X_1$.
5. To add interactions, use colons: $Y \sim X_1+X_2+X_1:X_2$.

This is incomplete: see, e.g., [this web page](https://faculty.chicagobooth.edu/richard.hahn/teaching/formulanotation.pdf) for fuller documentation.

Last Point: Linear Regression with Split Data
===

```{r}
load(url("https://raw.githubusercontent.com/pefreeman/36-290/master/EXAMPLE_DATASETS/PHOTO_MORPH/photo_morph.Rdata"))
```
The `load()` function places two stored variables into the global environment: <tt>predictors</tt>, a data frame with 16 measurements for each of 3,419 galaxies, and <tt>response</tt>, a vector with 3,419 redshifts (i.e., metrics of physical distance). 
```{r}
set.seed(404)
train = sample(nrow(predictors),0.7*nrow(predictors))
pred.train = predictors[train,]
pred.test  = predictors[-train,]
resp.train = response[train]
resp.test  = response[-train]

out.lm    = lm(resp.train~.,data=pred.train)    # Learn model
resp.pred = predict(out.lm,newdata=pred.test)   # Generate test-set predictions
mean((resp.pred-resp.test)^2)                   # Compute MSE
```

Last Point: Variance Inflation Factor (VIF)
===

An example:
```{r}
if ( require(car) == FALSE ) {
  install.packages("car",repos="https://cloud.r-project.org")
  library(car)
}
vif(out.lm)
```
We would remove the predictor with the largest VIF, so long as that VIF is greater than 5 or 10 (we'll use 10 here): `J.size`. (Then...`J.C`. Then all is fine.)
```{r}
out.lm    = lm(resp.train~.-J.size,data=pred.train)
vif(out.lm)
out.lm    = lm(resp.train~.-J.size-J.C,data=pred.train)
vif(out.lm)
resp.pred = predict(out.lm,newdata=pred.test)
mean((resp.pred-resp.test)^2)
```