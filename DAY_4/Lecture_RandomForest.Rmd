---
title: "Random Forest"
author: "Peter Freeman (2019 SLSW)"
date: "8 July 2019"
output: 
  slidy_presentation:
    font_adjustment: -1
---

Motivation
===

We have previously learned about decision trees. While trees are interpretable, they do have issues:

- They are highly variable. For instance, if you split a dataset in half and grow trees for each half, they can look very different.
- (Related:) They do not generalize as well as other models, i.e., they tend to have higher test-set MSE values.

To counteract these issues, one can utilize *bootstrap aggregation*, or *bagging*. Let's break this down:

- bootstrap: sample the training data *with replacement*
- aggregation: aggregate many trees, each constructed with a different bootstrap sample of the original training set

Examples of bootstrap samples:
```{r}
set.seed(505)
x = c(1,2,3,4)
sample(x,length(x),replace=TRUE)
sample(x,length(x),replace=TRUE)
```

Bagging: Algorithm
===

```
Specify number of trees: n

For each tree 1 -> n:
  - construct a bootstrap sample from the training data
  - grow a deep and unpruned tree (i.e., overfit!)
  
Pass test datum through all n trees:
  - if regression: overall prediction is average of those for each tree
  - if classification: by default, predicted class for each tree is the
                       most represented class in the leaf; overall prediction
                       is majority vote
```

Bagging improves prediction accuracy at the expense of interpretability: one can "read" a single tree, but if you have 500 trees, what can you do? 

$\Rightarrow$ You can use a metric dubbed *variable importance*, which is the average improvement in, e.g., RSS or Gini when splits are made on a particular predictor variable. The larger the average improvement, the more important the predictor variable.

Random Forest: Algorithm
===

The random forest algorithm *is* the bagging algorithm, with a tweak.

For each bootstrapped sampled dataset, we randomly select a subset of the predictor variables, and we build the tree using only those variables. By default, $m = \sqrt{p}$. (If $m = p$, then we recover the bagging algorithm.)

$\Rightarrow$ Selecting a variable subset for each tree allows us to get around the issue that if there is a dominant predictor variable, the first split is (almost always) made on that predictor variable. (Subsetting acts to "decorrelate" the different trees.)

Random Forest: Regression Example
===

The dataset loaded below contains brightness, distance, and mass information for 10,000 simulated galaxies. In our regression tree example, we will try to relate the measurements of brightness in six different bandpasses (`u`,...,`y`) to the measure of distance (`redshift`).
```{r}
df = read.csv("https://raw.githubusercontent.com/pefreeman/PSU_2019/master/PhotoZ.csv")
names(df)

set.seed(808)
s = sample(nrow(df),0.7*nrow(df))
pred.train = df[s,1:6]
pred.test  = df[-s,1:6]
resp.train = df[s,7]
resp.test  = df[-s,7]
```

Random Forest: Regression Example
===

```{r fig.height=6,fig.width=6,fig.align="center"}
if ( require(randomForest) == FALSE ) {
  install.packages("randomForest",repos="https://cloud.r-project.org")
  library(randomForest)
}

out.rf = randomForest(resp.train~.,data=pred.train,importance=TRUE)
resp.pred = predict(out.rf,newdata=pred.test)
mean((resp.pred-resp.test)^2)
importance(out.rf)
```
Note that `%IncMSE` is the preferred metric to refer to when ranking variables. For more information, see the extra notes on variable importance (`Extra_RF_VarImp`).

Random Forest: Regression Example
===

```{r fig.height=6,fig.width=6,fig.align="center"}
library(ggplot2)
ggplot(data=data.frame(resp.test,resp.pred),mapping=aes(x=resp.test,y=resp.pred)) + geom_point() +
  xlim(0,2) + ylim(0,2) + geom_abline(intercept=0,slope=1,color="red")
```

Random Forest: Regression Example
===

```{r fig.height=6,fig.width=6,fig.align="center"}
varImpPlot(out.rf)
```

Random Forest: Classification Example
===

The dataset that we read in below contains magnitude and redshift data for 500 quasars and 500 stars. The idea is to learn a classifier that can discriminate between quasars and stars with a low misclassification rate.
```{r}
df = read.csv("https://raw.githubusercontent.com/pefreeman/PSU_2019/master/StarQuasar.csv")
dim(df)
names(df)
set.seed(202)
s = sample(nrow(df),0.7*nrow(df))
pred.train = df[s,1:5]                                  # don't include redshift or redshift error!
pred.test  = df[-s,1:5]
resp.train = df[s,8]
resp.test  = df[-s,8]
```

Random Forest: Classification Example
===

```{r}
out.rf = randomForest(resp.train~.,data=pred.train)
resp.pred = predict(out.rf,newdata=pred.test)
(t = table(resp.pred,resp.test))
(t[1,2]+t[2,1])/sum(t)
importance(out.rf)
```

Random Forest: Classification Example
===

```{r fig.height=6,fig.width=6,fig.align="center"}
varImpPlot(out.rf)
```
