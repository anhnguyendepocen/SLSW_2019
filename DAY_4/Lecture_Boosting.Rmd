---
title: "Boosting"
author: "Peter Freeman (2019 SLSW)"
date: "8 July 2019"
output: 
  slidy_presentation:
    font_adjustment: -1
---

Context
===

"Can a set of weak learners create a single strong learner?"

$-$ Kearns & Valiant

An example of a "weak learner" is, e.g., a decision tree with a single split (i.e., a "decision stump"). The "set of weak learners" is, e.g., the repeated generation of stumps given some iterative rule, such as "let's upweight the currently misclassified observations next time around." (This is the core of the AdaBoost algorithm.) After iteration, a strong learner is created.

Boosting is a so-called "meta-algorithm": it is an algorithm that dictates how to repeatedly apply another algorithm. As such, boosting can be applied with many models, like linear regression. However, boosting is most associated with trees.

There are also many different kinds of boosting (i.e., many different ways to define the meta-algorithm: upweight observations the next time, etc.). The most oft-used boosting algorithm now, however, is *gradient boosting*, which we describe on the next slide.

Gradient Boosting
===

<center>![](http://www.stat.cmu.edu/~pfreeman/Algorithm_8.2.png){width=40%}</center>

(Algorithm 8.2, *Introduction to Statistical Learning* by James et al.)

The core idea of regression tree boosting: it *slowly* learns a model by fitting the *residuals* of the previous fit. In other words, it fits a stumpy tree to the original data, shrinks that tree (note the $\lambda$ parameter), updates the residuals, sets the data to be those residuals, and repeats. Hence each iteration of boosting attempts to hone in on those data that were not well fit previously.

The smaller the value of $\lambda$, the more slowly and conservatively the final tree is grown.

The contrast with bagging is that bagging involves growing many separate deep trees that are aggregated, while boosting grows one tree sequentially by adding a weighted series of stumps.

Gradient Boosting
===

<center>![](http://www.stat.cmu.edu/~pfreeman/boosting_1.png){width=40%} ![](http://www.stat.cmu.edu/~pfreeman/boosting_2.png){width=40%}</center>

<font size="2">
(From `https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d`)
</font>

Gradient Boosting: Regression Example
===

In an unechoed code chunk, we read in our brightness-morphology-distance galaxy dataset, the one with 16 predictor variables for each of 3419 galaxies.
```{r echo=FALSE}
load(url("https://raw.githubusercontent.com/pefreeman/36-290/master/EXAMPLE_DATASETS/PHOTO_MORPH/photo_morph.Rdata"))
set.seed(404)
train = sample(nrow(predictors),0.7*nrow(predictors))
pred.train = predictors[train,]
pred.test  = predictors[-train,]
resp.train = response[train]
resp.test  = response[-train]
```

To these data we apply *extreme gradient boosting*, or *xgboost*. The *xgboost* code is "prickly" in that its structure differs sufficiently from that of conventional statistical model code in `R`. You cannot just code an xgboost model the same way you'd code a linear regression model.
```{r}
if ( require(xgboost) == FALSE ) {
  install.packages("xgboost",repos="http://cloud.r-project.org")
  library(xgboost)
}
# combine predictors and response into a special type of R structure
train = xgb.DMatrix(data=as.matrix(pred.train),label=resp.train)
test  = xgb.DMatrix(data=as.matrix(pred.test),label=resp.test)
# perform cross-validation to tune parameters
#   objective="reg:linear" <-> use the mean-squared error to assess models
#   nrounds=30             <-> the maximum number of stumps to grow
#   nfold=5                <-> the number of CV folds (5 or 10, usually)
set.seed(101)
out.cv = xgb.cv(params=list(objective="reg:linear"),train,nrounds=30,nfold=5,verbose=0)
# if the optimal number of trees is equal to nrounds, increase nrounds and try again
cat("The optimal number of trees is ",which.min(out.cv$evaluation_log$test_rmse_mean))
```

Gradient Boosting: Regression Example
===

```{r fig.align="center",fig.width=4.5,fig.height=4.5}
# now that we know the number of trees, use it and learn the model with training data
out.xgb = xgboost(train,nrounds=which.min(out.cv$evaluation_log$test_rmse_mean),params=list(objective="reg:linear"),verbose=0)
pred    = predict(out.xgb,newdata=test)  # at least this looks similar to before...
mean((pred-resp.test)^2)

library(ggplot2)
ggplot(data=data.frame(resp.test,pred),mapping=aes(x=resp.test,y=pred)) + 
  geom_point() + xlim(range(resp.test)) + ylim(range(resp.test)) +
  geom_abline(intercept=0,slope=1,color="red")
```

Gradient Boosting: Variable Importance
===

Like random forest, boosting allows one to measure variable importance. For `xgboost`, this importance measure is called the "gain," defined as the "fractional contribution of each feature to the model based on the total gain of
this featureâ€™s splits. Higher percentage means a more important predictive feature."
```{r fig.align="center",fig.height=4.5,fig.width=4.5}
importance = xgb.importance(feature_names=names(pred.train),model=out.xgb)
xgb.plot.importance(importance_matrix=importance)
```
Note that one can also plot trees using `xgb.plot.tree()`. (Doing so requires the installation of the `DiagrammeR` package.)

Gradient Boosting: Binary Classification vs. Regression
===

The necessary code to do classification is similar to that need to perform regression. The differences are:

1. One has to change the response vector from a vector of factors (e.g., `QSO` and `STAR`) to a vector of numbers (`0` for `QSO`, etc.).
2. Instead of `objective="reg:linear"`, one uses `objective="binary:logistic"`.
3. The optimum number of trees, which is accessed via the variable `out.cv$evaluation_log$test_rmse_mean` for regression (where `out.cv` is the output of `xgb.cv()`), is accessed via the variable `out.cv$evaluation_log$test_error_mean` for classification.
4. Instead of using
```
pred = predict(xgb.out,newdata=test)
```
to generate predictions, one uses
```
prob = predict(xgb.out,newdata=test)
pred = ifelse(prob>0.5,1,0)
```