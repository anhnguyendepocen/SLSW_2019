---
title: "Decision Trees"
author: "Peter Freeman (2019 SLSW)"
date: "8 July 2019"
output: 
  slidy_presentation:
    font_adjustment: -1
---

Decision Tree: In Words
===

A decision tree is a model that segments a predictor space into disjoint $p$-dimensional hyper-rectangles, where $p$ is the number of predictor variables.

- For a regression tree, the predicted response in a hyper-rectangle is the average of the response values in that hyper-rectangle.

- For a classification tree, by default the predicted class in a hyper-rectangle is that class that is most represented in that hyper-rectangle.

<center>![](http://www.stat.cmu.edu/~pfreeman/Figure_8.3.png){width=60%}</center>

(Figure 8.3, *Introduction to Statistical Learning* by James et al.)

Decision Tree: Should I Use This Model?
===

Yes:

- It is easy to explain to non-statisticians.
- It is easy to visualize (and thus easy to interpret).

No:

- Trees do not generalize as well as other models (i.e., they tend to have higher test-set MSEs).

Decision Tree: Detail
===

<center>![](http://www.stat.cmu.edu/~pfreeman/Algorithm_8.1.png){width=50%}</center>

(Algorithm 8.1, *Introduction to Statistical Learning* by James et al.)

While the algorithm given above is for a regression tree, the classification tree algorithm is similar: instead of splits based on reduction of the residual sum-of-squares (RSS), the splits would be based on, e.g., reduction of the Gini coefficient, which is a metric that becomes smaller as each node becomes more "pure," i.e., populated more and more by objects of a single class.

In a perfect world, one would systematically try all possible combinations of hyper-rectangles to see which combination minimizes values of RSS/Gini. However, our world is imperfect; the decision tree algorithm is a greedy algorithm which utilizes top-down *recursive binary splitting* to build the model: while each split is a "locally optimal" one to make (i.e., it causes the largest reduction in RSS or Gini), the final model may not be "globally optimal" (i.e., it may not have the smallest possible overall RSS or Gini value).

To enlarge upon Step 1 above, splitting may cease not only when the number of data in a terminal node/hyper-rectangle is smaller than some threshold value, but also when the reduction in the RSS or Gini caused by splitting is smaller than some specified minimum value.

Decision Tree: Detail
===

<center>![](http://www.stat.cmu.edu/~pfreeman/Algorithm_8.1.png){width=50%}</center>

When building a decision tree, one must guard against overfitting. For instance, a tree that places a hyper-rectangle around each datum will be highly flexible (with training set MSE or MCR equal to zero!) but will not generalize well. One strategy for dealing with overfitting is to grow a large tree, then apply *cost complexity* (or *weakest link*) pruning (as described in Steps 2-4 above).

Regression Tree: Example
===

The dataset loaded below contains brightness, distance, and mass information for 10,000 simulated galaxies. In our regression tree example, we will try to relate the measurements of brightness in six different bandpasses (`u`,...,`y`) to the measure of distance (`redshift`).
```{r}
df = read.csv("https://raw.githubusercontent.com/pefreeman/PSU_2019/master/PhotoZ.csv")
names(df)

set.seed(808)
s = sample(nrow(df),0.7*nrow(df))
pred.train = df[s,1:6]
pred.test  = df[-s,1:6]
resp.train = df[s,7]
resp.test  = df[-s,7]
```

Regression Tree: Example
===

There is more than one package that one can use to learn a regression tree; here we use the `rpart` package.
```{r fig.height=6,fig.width=6,fig.align="center"}
if ( require(rpart) == FALSE ) {
  install.packages("rpart",repos="https://cloud.r-project.org")
  library(rpart)
}
if ( require(rpart.plot) == FALSE ) {
  install.packages("rpart.plot",repos="https://cloud.r-project.org")
  library(rpart.plot)
}

out.rpart = rpart(resp.train~.,data=pred.train)
resp.pred = predict(out.rpart,newdata=pred.test)
mean((resp.pred-resp.test)^2)
```

Regression Tree: Example
===

```{r fig.height=6,fig.width=6,fig.align="center"}
library(ggplot2)

ggplot(data=data.frame(resp.test,resp.pred),mapping=aes(x=resp.test,y=resp.pred)) + geom_point() +
  xlim(0,2) + ylim(0,2) + geom_abline(intercept=0,slope=1,color="red")
```

Regression Tree: Example
===

```{r fig.height=6,fig.width=6,fig.align="center"}
rpart.plot(out.rpart)
```

Classification Tree: Example
===

The dataset that we read in below contains magnitude and redshift data for 500 quasars and 500 stars. The idea is to learn a classifier that can discriminate between quasars and stars with a low misclassification rate.
```{r}
df = read.csv("https://raw.githubusercontent.com/pefreeman/PSU_2019/master/StarQuasar.csv")
dim(df)
names(df)
set.seed(202)
s = sample(nrow(df),0.7*nrow(df))
pred.train = df[s,1:5]                                  # don't include redshift or redshift error!
pred.test  = df[-s,1:5]
resp.train = df[s,8]
resp.test  = df[-s,8]
```

Classification Tree: Example
===

```{r}
out.rpart = rpart(resp.train~.,data=pred.train)
resp.pred = predict(out.rpart,newdata=pred.test,type="class")
(t = table(resp.pred,resp.test))
(t[1,2]+t[2,1])/sum(t)
```

Classification Tree: Example
===

```{r fig.height=6,fig.width=6,fig.align="center"}
rpart.plot(out.rpart)
```
